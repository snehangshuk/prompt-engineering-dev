# Section 2.4: Advanced Workflows - Prompt Templates

> **üìã For Circuit Users (No API Access Required)**
>
> This guide extracts the key prompts from `2.4-advanced-workflows.ipynb` into copy-paste templates for use in web-based AI chat interfaces like Cisco's internal Circuit.

| **Aspect** | **Details** |
|-------------|-------------|
| **Goal** | Master prompt chaining, tree of thoughts exploration, and LLM-as-judge evaluation tactics |
| **Time** | ~25 minutes |
| **Prerequisites** | Complete Sections 2.1‚Äì2.3 and be comfortable with reasoning patterns |
| **Next Steps** | Continue to Section 2.5: Hands-On Practice |

---

## üéØ Learning Objectives

By the end of this section, you'll be able to:
- ‚úÖ Break complex tasks into sequential prompt chains
- ‚úÖ Explore multiple solution approaches with tree-of-thoughts
- ‚úÖ Create evaluation rubrics with LLM-as-judge
- ‚úÖ Build self-correction loops for automated quality improvement

---

## üîó Tactic 6: Prompt Chaining

**Break complex tasks into sequential workflows**

Complex tasks can cause AI to "drop the ball" if handled in a single prompt. Prompt chaining breaks tasks into smaller, manageable subtasks where each step gets focused attention.

**The Analogy:**
- **Single Prompt:** One person doing everything (overwhelmed, drops details)
- **Prompt Chain:** Assembly line where each station specializes (focused, quality)

**When to Use Chaining:**
- ‚úÖ Multi-step code refactoring
- ‚úÖ Complex security audits
- ‚úÖ Documentation generation with multiple sections
- ‚úÖ Test generation following code analysis

**When NOT to Chain:**
- ‚ùå Simple, one-off tasks ("format this JSON")
- ‚ùå Quick questions ("what does this function do?")

---

### üìù Example 1: Simple 3-Step Chain for Code Review

**Step 1: Analyze Issues**

**Copy this to Circuit:**

```
Analyze this code for issues:

```python
def process_payment(amount, card_number):
    if amount > 0:
        charge_card(card_number, amount)
        return "success"
```

List:
- Security vulnerabilities
- Validation gaps
- Error handling issues
```

**Save the response!** You'll need it for Step 2.

---

**Step 2: Fix Based on Analysis**

**Copy this to Circuit (same chat or new):**

```
Fix this code based on analysis:

Original Code:
def process_payment(amount, card_number):
    if amount > 0:
        charge_card(card_number, amount)
        return "success"

Issues Found (from Step 1):
[PASTE THE ANALYSIS FROM STEP 1 HERE]

Provide the fixed code addressing all issues.
```

**Save the fixed code!** You'll use it for Step 3.

---

**Step 3: Generate Tests**

**Copy this to Circuit:**

```
Write tests for:

[PASTE THE FIXED CODE FROM STEP 2 HERE]

Include:
- Happy path test
- Edge cases (negative amounts, invalid cards)
- Error handling tests
```

**‚úÖ Benefits of Chaining:**
- Step 1 focuses ONLY on finding issues
- Step 2 focuses ONLY on fixing them
- Step 3 focuses ONLY on testing

Each step has full attention vs. trying to do everything at once!

---

### üìù Example 2: Self-Correcting Chain (Advanced)

This powerful pattern has AI review and improve its own work!

**Step 1: Generate Initial Code**

**Copy this to Circuit:**

```
Create a function to validate email addresses.

Requirements:
- Check format validity
- Handle edge cases
- Return clear error messages

Provide code in triple backticks.
```

---

**Step 2: AI Critiques Its Own Work**

**Copy this to Circuit (same chat):**

```
Now review YOUR OWN code above for issues.

Check for:
- Security vulnerabilities (e.g., regex denial of service)
- Edge cases not handled (empty strings, None, international emails)
- Missing validation (length limits, special characters)
- Code quality issues

List issues found. If none, say "No issues found".
```

---

**Step 3: AI Improves Based on Self-Critique**

**Copy this to Circuit (same chat):**

```
Based on the issues you just identified, provide improved code that fixes all problems.

Only change what's necessary to address the identified issues.
```

**üí° Key Insight:** By separating creation from critique, AI finds more issues and produces higher quality code!

---

## üéØ Activity 3.1: Try It Yourself - Prompt Chaining

**Scenario:** Review and fix code with security issues.

### Your Task
Create a 3-step chain:
1. **Analysis prompt:** Identify security issues only
2. **Fix prompt:** Address the issues from step 1
3. **Test prompt:** Generate tests for the fixed code

**Code to Review:**
```python
from flask import Flask, request
import sqlite3

@app.route('/user/<user_id>')
def get_user(user_id):
    conn = sqlite3.connect('users.db')
    cursor = conn.cursor()
    cursor.execute(f"SELECT * FROM users WHERE id = {user_id}")
    user = cursor.fetchone()
    conn.close()
    return jsonify({"id": user[0], "name": user[1]})
```

### Step 1: Analysis

**Copy this to Circuit:**

```
Analyze this code for security issues ONLY:

[PASTE CODE HERE]

List:
1. Security vulnerabilities with severity
2. Attack vectors
3. Potential exploits
```

---

### Step 2: Fix (using Step 1 output)

**Copy this to Circuit:**

```
Fix this code based on security analysis:

Original Code:
[PASTE ORIGINAL CODE]

Security Issues:
[PASTE STEP 1 ANALYSIS]

Provide secure code with:
- Parameterized queries
- Input validation
- Error handling
```

---

### Step 3: Tests (using Step 2 output)

**Copy this to Circuit:**

```
Write security tests for this fixed code:

[PASTE FIXED CODE FROM STEP 2]

Include tests for:
- SQL injection attempts
- Invalid input handling
- Edge cases
```

---

## üå≥ Tactic 7: Tree of Thoughts

**Explore multiple reasoning paths and select the best solution**

Instead of following a single path, Tree of Thoughts generates multiple candidate solutions, evaluates trade-offs, and selects the optimal approach.

**The Analogy:**
- **Single Prompt:** GPS giving you one route
- **Prompt Chain:** Following that one route step-by-step
- **Tree of Thoughts:** GPS showing you 3 routes (highway, scenic, fastest), comparing them, picking best

**When to Use:**
- ‚úÖ Architecture decisions (microservices vs monolith)
- ‚úÖ Algorithm selection (which data structure?)
- ‚úÖ Library choices (React vs Vue vs Svelte)
- ‚úÖ Design patterns (singleton vs factory vs DI)

---

### üìù Example 3: Algorithm Selection with ToT

**Step 1: Generate 3 Alternative Approaches**

**Copy this to Circuit:**

```
Problem: Find duplicate records in a dataset of 1 million user records

Generate 3 DIFFERENT approaches to solve this problem. For each approach, provide:
1. Algorithm/data structure used
2. High-level implementation strategy
3. Time and space complexity

Format:
APPROACH 1:
Name: [descriptive name]
Algorithm: [approach]
Strategy: [how it works]
Time: O(?)
Space: O(?)

APPROACH 2:
[same format]

APPROACH 3:
[same format]
```

---

**Step 2: Evaluate Trade-offs**

**Copy this to Circuit (same chat):**

```
Evaluate the 3 approaches you just proposed on these criteria:

- **Performance:** Time complexity impact on 1M records
- **Memory:** Space complexity and memory usage
- **Scalability:** How it handles growing data
- **Code Complexity:** Ease of implementation and maintenance
- **Edge Cases:** How well it handles duplicates, null values, etc.

Provide structured comparison in a table format with scores 1-10.
```

---

**Step 3: Select Best Approach**

**Copy this to Circuit (same chat):**

```
Based on your evaluation, select the BEST approach for finding duplicates in 1M records.

Provide:
1. Selected approach name
2. Justification:
   - Performance advantage: [specific reason]
   - Memory trade-off: [acceptable/optimal]
   - Best for: [use case specifics]
   - Wins on: [key criteria]
3. Python implementation of the selected approach
```

**‚úÖ Result:** You've explored alternatives, compared them objectively, and selected the best with clear reasoning!

---

## üéØ Activity 3.2: Try It Yourself - Tree of Thoughts

**Scenario:** Choose a caching strategy for your API.

### Your Task
Complete this 3-step tree-of-thoughts exploration:

**Step 1: Generate Alternatives**

**Copy this to Circuit:**

```
Generate 3 DIFFERENT caching strategies for an API:

1. Client-side caching (browser cache)
2. CDN caching (edge network)
3. Server-side caching (Redis/Memcached)

For each, describe:
- How it works
- What it caches
- TTL approach
- Invalidation strategy
```

---

**Step 2: Evaluate**

**Copy this to Circuit (same chat):**

```
Compare the 3 caching strategies on:

- **Latency reduction:** How much faster?
- **Database savings:** % of requests saved
- **Cost:** Infrastructure/bandwidth costs
- **Complexity:** Implementation difficulty
- **Invalidation:** How to handle stale data

Provide scores 1-10 for each criterion.
```

---

**Step 3: Select Winner**

**Copy this to Circuit (same chat):**

```
Recommend the BEST caching strategy (or hybrid approach) with:

- Clear justification
- When to use it
- Implementation priorities
- Potential pitfalls
```

---

## ‚öñÔ∏è Tactic 8: LLM-as-Judge

**Create evaluation rubrics and self-critique loops**

Use AI as a judge to evaluate and improve outputs‚Äîeither its own work or code from other sources.

**The 3 Patterns:**
1. **Single Evaluation:** "Is this code good enough?" (quality gate)
2. **Comparative:** "Which option is better?" (A vs B comparison)
3. **Self-Improvement:** "Generate ‚Üí Critique ‚Üí Fix" (automated quality loop)

**When to Use:**
- ‚úÖ Code review and quality assessment
- ‚úÖ Comparing multiple implementations
- ‚úÖ Automated QA workflows
- ‚úÖ Security audits with scoring

---

### üìù Example 4: Self-Improvement Loop (Pattern 3)

This is the most powerful pattern for automated quality improvement.

**Step 1: Generate Initial Solution**

**Copy this to Circuit:**

```
Create a function to validate email addresses.

Requirements:
- Check format validity
- Handle edge cases
- Return clear error messages

Provide code in triple backticks.
```

---

**Step 2: AI Judges Its Own Work**

**Copy this to Circuit (same chat):**

```
Review YOUR code above using these weighted criteria:

**Security (40%):**
- Input validation vulnerabilities
- Injection risks
- Data exposure issues

**Correctness (30%):**
- Edge cases (empty strings, None, special characters)
- Boundary conditions
- Logic errors

**Code Quality (30%):**
- Error handling completeness
- Code clarity and maintainability
- Following Python best practices

For each criterion:
- Score 0-10
- List specific issues
- Rate severity: CRITICAL / HIGH / MEDIUM / LOW

Provide scores and issues found.
```

---

**Step 3: AI Improves Based on Critique**

**Copy this to Circuit (same chat):**

```
Fix your code based on the issues you identified.

Address:
1. All CRITICAL issues first
2. Then HIGH severity
3. Then MEDIUM and LOW

Provide improved code with changes explained.
```

**üí° Result:** The code improves through self-reflection, catching issues that would have been missed in a single-shot approach!

---

### üìù Example 5: Comparative Evaluation (Pattern 2)

**Copy this to Circuit:**

```
You are a code quality judge. Evaluate based on:
- Security (40%): Resistance to attacks, proper crypto
- Performance (30%): Speed, resource usage
- Readability (30%): Clear, maintainable

Compare these password hashing implementations:

**Implementation A:**
def hash_pwd(p): return hashlib.md5(p.encode()).hexdigest()

**Implementation B:**
def hash_pwd(p): return bcrypt.hashpw(p.encode(), bcrypt.gensalt())

Provide:
- Scores for each criterion (0-10)
- Weighted total scores
- Recommendation with justification
- Why the winner is better
```

**‚úÖ Result:** Objective comparison with clear reasoning instead of vague "B is better"!

---

## üéØ Activity 3.3: Try It Yourself - LLM-as-Judge

**Scenario:** Evaluate and improve code automatically.

### Your Task
Create a self-improvement loop:

**Step 1: Generate**

**Copy this to Circuit:**

```
Create a function that validates and sanitizes user input for a SQL query.

Provide implementation with error handling.
```

---

**Step 2: Judge (with weighted criteria)**

**Copy this to Circuit (same chat):**

```
Critique YOUR implementation using weighted criteria:

**Security (60% weight) - CRITICAL:**
- SQL injection vulnerabilities
- Input validation gaps
- Sanitization effectiveness

**Best Practices (25% weight):**
- Use of parameterized queries
- Proper escaping methods
- Following secure coding standards

**Error Handling (15% weight):**
- Graceful failure on invalid input
- No information leakage in errors
- Clear error messages

For each criterion:
1. Score 0-10
2. List issues with severity (CRITICAL/HIGH/MEDIUM/LOW)
3. Explain the risk

Provide detailed critique.
```

---

**Step 3: Improve**

**Copy this to Circuit (same chat):**

```
Create improved implementation addressing ALL the issues you identified.

Prioritize:
1. CRITICAL security vulnerabilities
2. HIGH severity issues
3. Best practices improvements

Explain key security changes made.
```

---

## üéñÔ∏è Skill Tracker

After completing the activities above, check off the skills you've mastered:

### Prompt Chaining Skills
- [ ] **Skill #1:** I can break complex tasks into 3-5 sequential steps
- [ ] **Skill #2:** I can pass context between chain steps using structured outputs
- [ ] **Skill #3:** I can create self-correcting chains (generate ‚Üí critique ‚Üí improve)
- [ ] **Skill #4:** I understand when to chain vs. single prompt

### Tree of Thoughts Skills
- [ ] **Skill #5:** I can generate 3-4 alternative approaches to explore solution space
- [ ] **Skill #6:** I can create evaluation criteria with weighted scoring
- [ ] **Skill #7:** I can select best approach with clear justification
- [ ] **Skill #8:** I know when to use ToT vs. chaining (exploration vs. execution)

### LLM-as-Judge Skills
- [ ] **Skill #9:** I can create weighted evaluation rubrics (e.g., Security 60%, Quality 40%)
- [ ] **Skill #10:** I can implement self-improvement loops for automated quality
- [ ] **Skill #11:** I can do comparative evaluation (A vs B with clear winner)
- [ ] **Skill #12:** I can request severity ratings (CRITICAL/HIGH/MEDIUM/LOW)

**üèÜ Mastery Level:**
- **12/12 skills** = Expert! Ready for hands-on practice ‚úÖ
- **9-11/12 skills** = Proficient! Practice the missing skills
- **6-8/12 skills** = Good progress! Revisit examples above
- **0-5/12 skills** = Keep practicing! Try each example again

---

## üí° Key Takeaways

### Pattern Selection Guide

| Your Need | Best Pattern | When to Use |
|-----------|--------------|-------------|
| **Quality improvement** | Prompt Chaining | Most common (90% of tasks) |
| **Explore alternatives** | Tree of Thoughts | Critical decisions, architecture choices |
| **Automated QA** | LLM-as-Judge | Code review, testing, validation |

### The 90/10 Rule
- **90% of tasks:** Use Prompt Chaining (efficient, effective)
- **10% of tasks:** Use Tree of Thoughts when quality justifies the cost (2-3x more API calls)

### Best Practices Learned
1. **Chaining:** One clear goal per step, pass context forward with XML tags
2. **ToT:** Generate 3 alternatives, evaluate with weighted criteria, justify selection
3. **Judge:** Use weighted rubrics (e.g., Security 60%), request severity ratings
4. **Combine:** Chaining + Judge = powerful automated workflows

---

## ‚è≠Ô∏è Next Steps

<div style="background:rgb(12, 88, 160); padding: 16px; border-radius: 8px; border-left: 4px solid #3b82f6;">

**Continue to Section 2.5: Hands-On Practice**

Apply all 8 tactics independently:
- üéØ **Unguided practice activities** testing your mastery
- üìä **Self-evaluation checklists** to track progress
- üèÜ **Skill badges** for completing activities

[View 2.5-hands-on-practice.md](./2.5-hands-on-practice.md) ‚Üí

</div>

---

## ü§î Troubleshooting

**Q: My chain produces inconsistent results**
**A:** Make sure to:
1. Pass full context from previous steps (don't summarize)
2. Use XML tags to structure outputs for easy parsing
3. Be explicit: "Use the analysis from Step 1 above"

**Q: Tree of Thoughts takes too long**
**A:** It's meant to be slower but higher quality. Use it only for:
- Critical architecture decisions
- Long-term technology choices
- Problems where the wrong solution is costly

**Q: LLM-as-Judge gives vague feedback**
**A:** Strengthen your rubric:
1. Add specific weighted criteria (Security 40%, Performance 30%, etc.)
2. Request scores (0-10) for each criterion
3. Ask for severity ratings (CRITICAL/HIGH/MEDIUM/LOW)
4. Provide examples of good vs bad in the rubric

---

## üìö Related Resources

- [Claude Documentation - Chain Complex Prompts](https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/chain-prompts)
- [Tree of Thoughts Research](https://arxiv.org/abs/2305.10601)
- [OpenAI Evals Framework](https://github.com/openai/evals)
- [Full Interactive Notebook](./2.4-advanced-workflows.ipynb) (requires API access)

---

**üìù Note:** This template guide is a companion to the full Jupyter notebook. If you have API access (GitHub Copilot, OpenAI, Claude), use the interactive notebook for automated evaluation and progress tracking.
