{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3.4: Evaluate Your Prompt Templates\n",
    "\n",
    "| **Aspect** | **Details** |\n",
    "|-------------|-------------|\n",
    "| **Goal** | Add an evaluation layer that scores outputs from your prompt templates before they reach production |\n",
    "| **Time** | ~40 minutes |\n",
    "| **Prerequisites** | Sections 3.1‚Äì3.3 complete, `setup_utils.py` loaded |\n",
    "| **Level** | **Advanced** - Recommended after mastering 3.2 & 3.3 |\n",
    "| **What You'll Strengthen** | Trustworthy automation, rubric design, quality gates |\n",
    "| **Next Steps** | Return to the [Module 3 overview](./README.md) or wire scores into your workflow |\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° New to this module?** This is an **advanced optional section**. If you haven't completed Sections 3.2 and 3.3, go back and master those first. This section builds on that foundation.\n",
    "\n",
    "You just built reusable prompt templates in Sections 3.2 and 3.3. Now you'll learn how to **evaluate those AI outputs** with weighted rubrics so you can accept great responses, request revisions, or escalate risky ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Setup Check\n",
    "\n",
    "Since you completed Section 1, setup is already done! We just need to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick setup check - imports setup_utils\n",
    "try:\n",
    "    import importlib\n",
    "    import setup_utils\n",
    "    importlib.reload(setup_utils)\n",
    "    from setup_utils import *\n",
    "    print(f\"‚úÖ Setup loaded! Using {get_provider().upper()} with {get_default_model()}\")\n",
    "    print(\"üöÄ Ready to score AI outputs with evaluation rubrics!\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Setup not found!\")\n",
    "    print(\"üí° Please run 3.1-setup-and-introduction.ipynb first to set up your environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Template\n",
    "\n",
    "### Building the Evaluation Loop for Your Prompt Templates\n",
    "\n",
    "<div style=\"background:#fef3c7; border-left:4px solid #f59e0b; padding:16px; border-radius:6px; margin:20px 0; color:#000000;\">\n",
    "<strong style=\"color:#92400e;\">üéØ What You'll Build in This Section</strong><br><br>\n",
    "\n",
    "You'll create an **evaluation rubric** that reviews the output produced by your prompt templates. The rubric scores the response, explains its verdict, and tells you whether to accept it, request a revision, or fall back to a human reviewer.\n",
    "<br><br>\n",
    "<strong>Time Required:</strong> ~40 minutes (learning + examples + activity)\n",
    "</div>\n",
    "\n",
    "Layering an evaluation rubric after your templates keeps quality high without sending everything back to humans. In Module 2 we learned that traditional metrics (F1, BLEU, ROUGE) miss hallucinations and manual reviews are too slow to scale. A rubric-driven evaluation gives you semantic understanding *and* consistent scoring.\n",
    "\n",
    "---\n",
    "\n",
    "#### ü§î Quick Reflection: Your Quality Assurance Experience\n",
    "\n",
    "Before we dive in, take a moment to reflect on your own experience:\n",
    "\n",
    "<div style=\"background:#e0f2fe; border-left:4px solid #0284c7; padding:16px; border-radius:6px; margin:16px 0; color:#000000;\">\n",
    "<strong style=\"color:#0c4a6e;\">üí≠ Think about the last time you reviewed AI-generated content or code:</strong><br><br>\n",
    "\n",
    "**Question 1:** How did you decide if the output was \"good enough\"? Gut feeling? Checklist?<br>\n",
    "**Question 2:** Did different reviewers accept/reject the same output differently?<br>\n",
    "**Question 3:** Could you articulate why you approved or rejected it to someone else?\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### The Problems We're Solving Together\n",
    "\n",
    "Sound familiar? Let's connect these challenges to real scenarios you've probably experienced:\n",
    "\n",
    "<div style=\"background:#fff; border:2px solid #e5e7eb; padding:16px; border-radius:8px; margin:16px 0; color:#000000;\">\n",
    "\n",
    "**1. üö® Silent Failures**\n",
    "\n",
    "<div style=\"margin-left:16px; padding:12px; background:#fef3c7; border-radius:6px; margin-top:8px; margin-bottom:16px;\">\n",
    "<strong>üîç Spot This Pattern?</strong><br><br>\n",
    "‚Ä¢ Your AI code review template flags 5 security issues<br><br>\n",
    "‚Ä¢ You merge it, deploy to production<br><br>\n",
    "‚Ä¢ Later: Customer reports a security vulnerability the AI review mentioned but... got the line numbers wrong<br><br>\n",
    "‚Ä¢ Result: The review looked comprehensive but had factual errors no one caught<br><br>\n",
    "<strong>Result:</strong> Polished-looking output with hidden mistakes that traditional metrics can't detect.\n",
    "</div>\n",
    "\n",
    "**Real impact:**\n",
    "- AI-generated code reviews that miss critical bugs\n",
    "- Documentation that looks complete but has incorrect examples\n",
    "- Test specifications that skip edge cases\n",
    "- **Your risk:** How do you know if AI output is production-ready?\n",
    "\n",
    "---\n",
    "\n",
    "**2. ‚è≥ Manual QA Bottlenecks**\n",
    "\n",
    "<div style=\"margin-left:16px; padding:12px; background:#fef3c7; border-radius:6px; margin-top:8px; margin-bottom:16px;\">\n",
    "<strong>üîç Sound Familiar?</strong><br><br>\n",
    "‚Ä¢ You generate 50 AI code reviews per day<br><br>\n",
    "‚Ä¢ Senior engineer spot-checks 5 of them (10% sample)<br><br>\n",
    "‚Ä¢ Pipeline is blocked waiting for manual validation<br><br>\n",
    "‚Ä¢ Meanwhile: 45 reviews ship without verification<br><br>\n",
    "<strong>Result:</strong> Either bottleneck the pipeline or accept unvalidated outputs.\n",
    "</div>\n",
    "\n",
    "**The scaling problem:**\n",
    "- Human review doesn't scale to hundreds of AI outputs daily\n",
    "- Spot checks miss systemic issues\n",
    "- Feedback arrives too late for CI/CD pipelines\n",
    "- **Ask yourself:** Can you manually verify every AI-generated output?\n",
    "\n",
    "---\n",
    "\n",
    "**3. üéØ Inconsistent Standards**\n",
    "\n",
    "<div style=\"margin-left:16px; padding:12px; background:#fef3c7; border-radius:6px; margin-top:8px; margin-bottom:16px;\">\n",
    "<strong>üîç Ever Been Here?</strong><br><br>\n",
    "‚Ä¢ Engineer A accepts AI code review if it mentions security<br><br>\n",
    "‚Ä¢ Engineer B wants specific line numbers and fix recommendations<br><br>\n",
    "‚Ä¢ Engineer C rejects anything without performance analysis<br><br>\n",
    "‚Ä¢ New hire: \"What's our acceptance criteria?\"<br><br>\n",
    "‚Ä¢ Team lead: *crickets*<br><br>\n",
    "<strong>Result:</strong> Every reviewer applies different standards, inconsistent quality.\n",
    "</div>\n",
    "\n",
    "**The consistency problem:**\n",
    "- No codified criteria for \"good enough\"\n",
    "- Different reviewers = different thresholds\n",
    "- Teams struggle to know when to ship vs regenerate\n",
    "- **Challenge:** Try defining your acceptance criteria right now. How specific can you be?\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### Here's What You'll Build Today\n",
    "\n",
    "By the end of this section, you'll have an evaluation template that:\n",
    "\n",
    "‚úÖ **Catches hidden errors** - Semantic evaluation detects factually wrong but well-formatted outputs<br>\n",
    "‚úÖ **Scales automatically** - Review hundreds of AI outputs without human bottlenecks<br>\n",
    "‚úÖ **Applies consistent criteria** - Same rubric, same thresholds, every evaluation<br>\n",
    "‚úÖ **Provides actionable verdicts** - Accept/Revise/Reject decisions your pipeline can automate<br>\n",
    "‚úÖ **Documents reasoning** - Auditable scores with rationale for every decision\n",
    "\n",
    "**Ready to build it?** Let's turn those quality gaps into systematic evaluation. ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíª Working Example: Judge the Section 3.2 Code Review\n",
    "\n",
    "> **Note:** To avoid the AI model grading its own review or automatically preferring its own output, we switch the judge to a different provider/model so the evaluation comes from an independent model.\n",
    "\n",
    "This cell replays the Section 3.2 template to generate the comprehensive AI review, then immediately scores it with the judge using the same monthly report diff.\n",
    "\n",
    "**What you'll see:**\n",
    "- The full AI review that the template produces\n",
    "- How the rubric weights accuracy, completeness, actionability, and communication\n",
    "- An Accept/Revise/Reject recommendation tied to the numeric thresholds\n",
    "\n",
    "<div style=\"margin-top:16px; padding:16px; background:#fef3c7; border-left:4px solid #f59e0b; border-radius:8px; color:#78350f;\">\n",
    "<strong>‚ö†Ô∏è Heads-up:</strong> <br>\n",
    "The next cell first replays the Section 3.2 prompt template to regenerate the AI review, then runs the evaluation rubric on that fresh output.\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-top:16px; color:#991b1b; padding:12px; background:#fee2e2; border-radius:6px; border-left:4px solid #ef4444;\">\n",
    "<strong>‚ö†Ô∏è IMPORTANT:</strong><br>\n",
    "To avoid the AI model grading its own review or automatically preferring its own output, we switch the judge to a different provider/model so the evaluation comes from an independent model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Judge the Section 3.2 code review output\n",
    "\n",
    "code_diff = '''\n",
    "+ import json\n",
    "+ import time\n",
    "+ from decimal import Decimal\n",
    "+\n",
    "+ CACHE = {}\n",
    "+\n",
    "+ def generate_monthly_report(org_id, db, s3_client):\n",
    "+     if org_id in CACHE:\n",
    "+         return CACHE[org_id]\n",
    "+\n",
    "+     query = f\"SELECT * FROM invoices WHERE org_id = '{org_id}' ORDER BY created_at DESC\"\n",
    "+     rows = db.execute(query)\n",
    "+\n",
    "+     total = Decimal(0)\n",
    "+     items = []\n",
    "+     for row in rows:\n",
    "+         total += Decimal(row['amount'])\n",
    "+         items.append({\n",
    "+             'id': row['id'],\n",
    "+             'customer': row['customer_name'],\n",
    "+             'amount': float(row['amount'])\n",
    "+         })\n",
    "+\n",
    "+     payload = {\n",
    "+         'org': org_id,\n",
    "+         'generated_at': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "+         'total': float(total),\n",
    "+         'items': items\n",
    "+     }\n",
    "+\n",
    "+     key = f\"reports/{org_id}/{int(time.time())}.json\"\n",
    "+     time.sleep(0.5)\n",
    "+     s3_client.put_object(\n",
    "+         Bucket='company-reports',\n",
    "+         Key=key,\n",
    "+         Body=json.dumps(payload),\n",
    "+         ACL='public-read'\n",
    "+     )\n",
    "+\n",
    "+     CACHE[org_id] = key\n",
    "+     return key\n",
    "'''\n",
    "\n",
    "review_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You follow structured review templates and produce clear, actionable findings.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "<role>\n",
    "Act as a Senior Software Engineer specializing in Python backend services.\n",
    "Your expertise covers security best practices, performance tuning, reliability, and maintainable design.\n",
    "</role>\n",
    "\n",
    "<context>\n",
    "Repository: analytics-platform\n",
    "Service: Reporting API\n",
    "Purpose: Add a monthly invoice report exporter that finance can trigger\n",
    "Change Scope: Review focuses on the generate_monthly_report implementation\n",
    "Language: python\n",
    "</context>\n",
    "\n",
    "<code_diff>\n",
    "{code_diff}\n",
    "</code_diff>\n",
    "\n",
    "<review_guidelines>\n",
    "Assess the change across multiple dimensions:\n",
    "\n",
    "1. Security ‚Äî SQL injection, S3 object exposure, sensitive data handling.\n",
    "2. Performance ‚Äî query efficiency, blocking calls, caching behaviour.\n",
    "3. Error Handling ‚Äî resilience to empty results, network/storage failures.\n",
    "4. Code Quality ‚Äî readability, global state, data conversions.\n",
    "5. Correctness ‚Äî totals, currency precision, repeated report generation.\n",
    "6. Best Practices ‚Äî configuration management, separation of concerns, testing hooks.\n",
    "For each finding, cite the diff line, describe impact, and share an actionable fix.\n",
    "</review_guidelines>\n",
    "\n",
    "<tasks>\n",
    "Step 1 - Think: Analyse the diff using the dimensions listed above.\n",
    "Step 2 - Assess: For each issue, capture Severity (CRITICAL/MAJOR/MINOR/INFO), Category, Line, Issue, Impact.\n",
    "Step 3 - Suggest: Provide a concrete remediation (code change or process tweak).\n",
    "Step 4 - Verdict: Summarise overall risk and recommend APPROVE / REQUEST CHANGES / NEEDS WORK.\n",
    "</tasks>\n",
    "\n",
    "<output_format>\n",
    "## Code Review Summary\n",
    "Write one paragraph on overall health and primary risks\n",
    "\n",
    "## Findings\n",
    "For each finding, use this structure:\n",
    "\n",
    "### {{SEVERITY}} Issue Title\n",
    "**Category:** Security / Performance / Quality / Correctness / Best Practices\n",
    "**Line:** Cite the line number\n",
    "**Issue:** Describe the impact in clear terms\n",
    "**Recommendation:**\n",
    "```\n",
    "# Provide safer / faster / cleaner fix here\n",
    "```\n",
    "\n",
    "## Overall Assessment\n",
    "**Recommendation:** APPROVE | REQUEST CHANGES | NEEDS WORK\n",
    "**Summary:** Explain what to address before merge\n",
    "</output_format>\n",
    "\"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"üîç Generating the Section 3.2 code review...\")\n",
    "print(f\"Using {get_provider().upper()} with {get_default_model()}\")\n",
    "print(\"=\" * 70)\n",
    "ai_generated_review = get_chat_completion(review_messages, temperature=0.0)\n",
    "print(ai_generated_review)\n",
    "print(\"=\" * 70)\n",
    "\n",
    "rubric_prompt = \"\"\"\n",
    "<context>\n",
    "Original pull request diff:\n",
    "{context}\n",
    "\n",
    "AI-generated review to evaluate:\n",
    "{ai_output}\n",
    "</context>\n",
    "\n",
    "<rubric>\n",
    "1. Accuracy (40%): Do identified issues actually exist and are correctly described?\n",
    "2. Completeness (30%): Are major concerns covered? Any critical issues missed?\n",
    "3. Actionability (20%): Are recommendations specific and implementable?\n",
    "4. Communication (10%): Is the review professional, clear, and well-structured?\n",
    "</rubric>\n",
    "\n",
    "<instructions>\n",
    "Score each criterion 1-5 with detailed rationale.\n",
    "Calculate weighted total: (Accuracy√ó0.4) + (Completeness√ó0.3) + (Actionability√ó0.2) + (Communication√ó0.1)\n",
    "\n",
    "Recommend:\n",
    "- ACCEPT (‚â•3.5): Production-ready\n",
    "- REVISE (2.5-3.4): Needs improvements  \n",
    "- REJECT (<2.5): Unacceptable quality\n",
    "</instructions>\n",
    "\n",
    "Provide structured evaluation with scores, weighted total, recommendation, and feedback.\n",
    "\"\"\"\n",
    "\n",
    "judge_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a Principal Engineer reviewing AI-generated code feedback.\"},\n",
    "    {\"role\": \"user\", \"content\": rubric_prompt.format(context=code_diff, ai_output=ai_generated_review)}\n",
    "]\n",
    "\n",
    "original_provider = setup_utils.get_provider()\n",
    "try:\n",
    "    setup_utils.set_provider('openai')\n",
    "    print(\"‚öñÔ∏è JUDGE EVALUATION IN PROGRESS...\")\n",
    "    print(f\"Using {get_provider().upper()} with {get_default_model()}\")\n",
    "    print(\"=\" * 70)\n",
    "    judge_result = get_chat_completion(judge_messages, temperature=0.0)\n",
    "    print(judge_result)\n",
    "    print(\"=\" * 70)\n",
    "finally:\n",
    "    setup_utils.set_provider(original_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üèóÔ∏è Understanding What You Just Saw: The Tactical Combination\n",
    "\n",
    "Now that you've seen the judge in action, let's understand how combining tactics from Module 2 creates a reliable evaluation system.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Add a Judge After Prompt Templates?\n",
    "\n",
    "Before diving into tactics, let's understand the value:\n",
    "\n",
    "- **Detect hidden errors:** LLM judges evaluate meaning, not just surface patterns. Paraphrased but wrong answers score poorly even when traditional metrics look fine.\n",
    "- **Scale automatically:** A second AI call verifies template outputs meet criteria every time‚Äîno human bottleneck for hundreds of daily reviews.\n",
    "- **Accelerate iteration:** Scores highlight which tactic block needs improvement, letting you A/B test prompts without waiting for manual QA.\n",
    "\n",
    "---\n",
    "\n",
    "#### The 6-Tactic Recipe (With the \"Why\" Behind Each)\n",
    "\n",
    "Here's how we combine tactics strategically to solve specific failure modes:\n",
    "\n",
    "<div style=\"overflow-x:auto;\">\n",
    "<table style=\"width:100%; border-collapse:collapse; margin:16px 0; background:#fff; border:2px solid #e5e7eb; color:#000000; table-layout:fixed;\">\n",
    "<tr style=\"background:#f8fafc; font-weight:bold; color:#000000;\">\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; width:25%; word-wrap:break-word;\">Tactic</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; width:35%; word-wrap:break-word;\">What It Fixes</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; width:40%; word-wrap:break-word;\">Why LLMs Need This</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\"><strong>üé≠ Role Prompting</strong></td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\">Generic \"good/bad\" judgments</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\">Positions as Principal Engineer ‚Üí Expert evaluation</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\"><strong>üì¶ Structured Inputs (XML)</strong></td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\">Judge mixes submission with criteria</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\">Clear boundaries ‚Üí Model knows what vs how</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\"><strong>üî¢ Rubric Decomposition</strong></td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\">Inconsistent scoring across runs</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\">Weighted criteria ‚Üí Systematic evaluation</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\"><strong>üß† Chain-of-Thought</strong></td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\">\"3/5\" scores without rationale</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\">Evidence-based reasoning ‚Üí Auditable</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\"><strong>üìä Decision Thresholds</strong></td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\">No clear automation hook</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\">Numeric gates ‚Üí Accept/Revise/Reject</td>\n",
    "</tr>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "**üí° Key Insight:** Each tactic removes one type of evaluation failure. Combine them, and you get reliable, scalable quality gates.\n",
    "\n",
    "---\n",
    "\n",
    "#### See the Difference: With vs Without Tactics\n",
    "\n",
    "<div style=\"overflow-x:auto;\">\n",
    "<table style=\"width:100%; border-collapse: collapse; margin:16px 0; background:#fff; border:2px solid #e5e7eb; color:#000000; table-layout:fixed;\">\n",
    "<tr style=\"background:#f8fafc; font-weight:bold; color:#000000;\">\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; width:20%; word-wrap:break-word;\">Scenario</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; width:40%; word-wrap:break-word;\">‚ùå Without Tactics</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; width:40%; word-wrap:break-word;\">‚úÖ With Tactics</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\"><strong>Code Review Evaluation</strong></td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; background:#fef2f2; color:#000000; word-wrap:break-word;\">\n",
    "\"This review looks comprehensive. 7/10.\"\n",
    "<br><span style=\"color:#991b1b;\">‚Üí No rationale, unclear why 7/10</span>\n",
    "</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; background:#ecfdf5; color:#000000; word-wrap:break-word;\">\n",
    "<strong>Accuracy: 4/5</strong> (All issues exist)<br>\n",
    "<strong>Completeness: 3/5</strong> (Missed performance)<br>\n",
    "<strong>Weighted: 3.4/5 ‚Üí REVISE</strong><br>\n",
    "<strong>Feedback:</strong> Add performance section\n",
    "<br><span style=\"color:#166534;\">‚Üí Specific, actionable, auditable</span>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\"><strong>Inconsistent Scores</strong></td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; background:#fef2f2; color:#000000; word-wrap:break-word;\">\n",
    "Run 1: 8/10 \"Good work\"<br>\n",
    "Run 2: 6/10 \"Needs work\"<br>\n",
    "Run 3: 7/10 \"Acceptable\"\n",
    "<br><span style=\"color:#991b1b;\">‚Üí Same input, different scores</span>\n",
    "</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; background:#ecfdf5; color:#000000; word-wrap:break-word;\">\n",
    "Every run evaluates:<br>\n",
    "Accuracy (40%) ‚Üí Completeness (30%) ‚Üí Actionability (20%) ‚Üí Communication (10%)\n",
    "<br><span style=\"color:#166534;\">‚Üí Consistent criteria every time</span>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\"><strong>Pipeline Automation</strong></td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; background:#fef2f2; color:#000000; word-wrap:break-word;\">\n",
    "\"This could be better but it's okay\"\n",
    "<br><br>\n",
    "<span style=\"color:#991b1b;\">‚Üí Can't automate ambiguous output</span>\n",
    "</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; background:#ecfdf5; color:#000000; word-wrap:break-word;\">\n",
    "<strong>ACCEPT</strong> (3.6/5 ‚â• 3.5 threshold)\n",
    "<br><br>\n",
    "<span style=\"color:#166534;\">‚Üí Clear automation hook</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Weighted Rubrics?\n",
    "\n",
    "A single \"Is this good?\" question lets hidden errors slip through. **Weighted rubrics** give you:\n",
    "\n",
    "- **Multi-dimensional evaluation:** Accuracy, completeness, actionability, communication\n",
    "- **Prioritization:** Weight critical criteria higher (e.g., Accuracy 40%, Communication 10%)\n",
    "- **Measurable thresholds:** Clear numeric gates for automation decisions\n",
    "- **Auditable feedback:** Every score includes evidence and rationale\n",
    "\n",
    "<div style=\"margin-top:12px; padding:12px; background:#fef3c7; border-radius:6px; color:#78350f;\">\n",
    "<strong>üß™ In This Tutorial:</strong> We use 4-criterion weighted rubrics (accuracy, completeness, actionability, communication). Feel free to adjust weights based on your domain‚Äîsecurity-critical systems might weight accuracy 50%.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Breaking Down the Judge Template: A Walkthrough\n",
    "\n",
    "Let's dissect the judge template you saw in action. We'll walk through each block and see how it uses Module 2 tactics:\n",
    "\n",
    "---\n",
    "\n",
    "#### Block 1: üé≠ Set the Judge Persona\n",
    "\n",
    "**What it looked like:**\n",
    "```xml\n",
    "<role>\n",
    "You are a Principal Engineer reviewing AI-generated code feedback.\n",
    "</role>\n",
    "```\n",
    "\n",
    "**What this does:** Activates expert evaluation standards. Instead of generic \"looks good/bad\" judgments, you get analysis a Principal Engineer would apply‚Äîunderstanding what makes feedback production-ready vs. needing revision.\n",
    "\n",
    "**Module 2 Tactic:** Role Prompting\n",
    "\n",
    "---\n",
    "\n",
    "#### Block 2: üî¢ Define Weighted Rubric (What to Evaluate)\n",
    "\n",
    "**What it looked like:**\n",
    "```xml\n",
    "<rubric>\n",
    "1. Accuracy (40%): Do identified issues actually exist and are correctly described?\n",
    "2. Completeness (30%): Are major concerns covered? Any critical issues missed?\n",
    "3. Actionability (20%): Are recommendations specific and implementable?\n",
    "4. Communication (10%): Is the review professional, clear, and well-structured?\n",
    "</rubric>\n",
    "```\n",
    "\n",
    "**What this does:** Creates a systematic multi-dimensional checklist with explicit priorities. Accuracy gets highest weight (40%) because factually wrong reviews are worse than poorly formatted ones. Every evaluation checks ALL 4 dimensions‚Äîno skipped criteria.\n",
    "\n",
    "**Module 2 Tactic:** Task Decomposition + Weighted Criteria\n",
    "\n",
    "---\n",
    "\n",
    "#### Block 3: üß† Guide the Evaluation Process (How to Score)\n",
    "\n",
    "**What it looked like:**\n",
    "```xml\n",
    "<instructions>\n",
    "Score each criterion 1-5 with detailed rationale:\n",
    "- 5: Excellent - Exceeds expectations\n",
    "- 4: Good - Meets expectations with minor gaps\n",
    "- 3: Acceptable - Meets minimum bar\n",
    "- 2: Needs work - Significant gaps\n",
    "- 1: Unacceptable - Fails to meet standards\n",
    "\n",
    "Calculate weighted total: (Accuracy√ó0.4) + (Completeness√ó0.3) + (Actionability√ó0.2) + (Communication√ó0.1)\n",
    "\n",
    "Recommend:\n",
    "- ACCEPT (‚â•3.5): Production-ready\n",
    "- REVISE (2.5-3.4): Needs improvements, provide specific guidance\n",
    "- REJECT (<2.5): Start over with different approach\n",
    "</instructions>\n",
    "```\n",
    "\n",
    "**What this does:** Forces the judge to show its reasoning. You don't get \"3/5\" scores without explanation‚Äîyou get evidence-based rationale tied to the explicit scale. Weighted calculation and thresholds make decisions consistent and automatable.\n",
    "\n",
    "**Module 2 Tactic:** Chain-of-Thought + Decision Thresholds\n",
    "\n",
    "---\n",
    "\n",
    "#### Block 4: üì¶ Separate Submission from Criteria\n",
    "\n",
    "**What it looked like:**\n",
    "```xml\n",
    "<submission>\n",
    "{{llm_output_under_review}}\n",
    "</submission>\n",
    "```\n",
    "\n",
    "**What this does:** Clear XML boundaries separate \"what to evaluate\" from \"how to evaluate it.\" The judge knows the submission content is what needs scoring, not the rubric criteria themselves.\n",
    "\n",
    "**Module 2 Tactic:** Structured Inputs (XML)\n",
    "\n",
    "---\n",
    "\n",
    "#### Block 5: üìä Specify Output Format (How to Report)\n",
    "\n",
    "**What it looked like:**\n",
    "```xml\n",
    "<output_format>\n",
    "Provide structured evaluation with:\n",
    "- Individual scores (1-5) with rationale for each criterion\n",
    "- Weighted total score\n",
    "- Recommendation (ACCEPT/REVISE/REJECT)\n",
    "- Specific feedback for improvements\n",
    "</output_format>\n",
    "```\n",
    "\n",
    "**What this does:** Standardizes output for automation. Your pipeline can parse the ACCEPT/REVISE/REJECT decision, extract numeric scores for tracking, and surface improvement feedback. No more free-form text that's hard to act on.\n",
    "\n",
    "**Module 2 Tactic:** Structured Output\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÑ Making It Reusable\n",
    "\n",
    "**Add variables** for the parts that change between use cases:\n",
    "\n",
    "```xml\n",
    "<role>\n",
    "You are a {{judge_role}} reviewing {{content_type}}.\n",
    "</role>\n",
    "\n",
    "<rubric>\n",
    "1. {{criterion_1_name}} ({{weight_1}}%): {{criterion_1_description}}\n",
    "2. {{criterion_2_name}} ({{weight_2}}%): {{criterion_2_description}}\n",
    "...\n",
    "</rubric>\n",
    "```\n",
    "\n",
    "Now you can use the same judge template across different domains:\n",
    "- Code reviews: `judge_role=\"Principal Engineer\"`, `content_type=\"AI-generated code feedback\"`\n",
    "- Documentation: `judge_role=\"Technical Writer\"`, `content_type=\"API documentation\"`\n",
    "- Test specs: `judge_role=\"QA Lead\"`, `content_type=\"test specifications\"`\n",
    "\n",
    "**One judge template, infinite use cases.** Just adjust the role, criteria, and weights to match your domain.\n",
    "\n",
    "---\n",
    "\n",
    "#### Design Principles for Rubrics\n",
    "\n",
    "**1. Weighted Criteria** ‚Äì Prioritize what matters most (e.g., accuracy first for safety-critical domains).\n",
    "\n",
    "**2. Explicit Scale** ‚Äì Clear 1-5 definitions stop the judge from drifting between runs.\n",
    "\n",
    "**3. Evidence-Based Rationale** ‚Äì Forces the model to ground scores in the submission content.\n",
    "\n",
    "**4. Actionable Thresholds** ‚Äì Numeric gates (3.5, 2.5) enable pipeline automation.\n",
    "\n",
    "**5. Improvement Guidance** ‚Äì \"Revise\" outcomes must include next steps for the generator.\n",
    "\n",
    "---\n",
    "\n",
    "#### Calibration: Keeping Scores Consistent\n",
    "\n",
    "The rubric defines **what** to score; calibration ensures **how** it's scored stays consistent. Instead of generic \"7/10 - pretty good\" language, define anchors:\n",
    "\n",
    "**Example:** 7/10 = factually accurate with minor gaps, clear structure, appropriate for target audience, missing 1-2 implementation details.\n",
    "\n",
    "<div style=\"overflow-x:auto;\">\n",
    "<table style=\"width:100%; border-collapse:collapse; margin:16px 0; background:#fff; border:2px solid #e5e7eb; color:#000000; table-layout:fixed;\">\n",
    "<tr style=\"background:#f8fafc; font-weight:bold; color:#000000;\">\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; width:25%; word-wrap:break-word;\">Scenario</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; width:25%; word-wrap:break-word;\">9/10 (Excellent)</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; width:25%; word-wrap:break-word;\">5/10 (Acceptable)</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; width:25%; word-wrap:break-word;\">2/10 (Needs Work)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\">Technical documentation</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\">Complete, tested, handles edge cases</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\">Covers main flows, some gaps</td>\n",
    "<td style=\"padding:8px; border:1px solid #e5e7eb; color:#000000; word-wrap:break-word;\">Basic concepts only, missing details</td>\n",
    "</tr>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "**Best Practices:**\n",
    "- **Anchor scores** with real examples at each level (1, 3, 5)\n",
    "- **Recalibrate quarterly** with domain experts as standards evolve\n",
    "- **Check inter-rater reliability** to ensure consistent interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Activity 3.4: Create Your Judge Template\n",
    "\n",
    "Now that you've seen how the template works, try building one from scratch for a cache refactor scenario.\n",
    "\n",
    "**Your task:** Create an evaluation template that evaluates AI-generated refactor explanations. Open **[`activities/activity-3.4-evaluation-templates.md`](./activities/activity-3.4-evaluation-templates.md)** and complete the template between the `<!-- TEMPLATE START -->` and `<!-- TEMPLATE END -->` markers.\n",
    "\n",
    "The template should:\n",
    "- Set an appropriate judge role (e.g., Senior Engineer reviewing refactor proposals)\n",
    "- Define weighted rubric criteria (Correctness, Design, Safety, Tests)\n",
    "- Include explicit scoring scale (1-5) with decision thresholds\n",
    "- Specify structured output format with verdict and improvement feedback\n",
    "\n",
    "**The challenge:** The refactor scenario includes subtle issues. Your judge should catch factual inaccuracies, missing test coverage, and design trade-offs.\n",
    "\n",
    "When you're done, come back and run the cell below to test it. Compare your result with the **[solution](./solutions/activity-3.4-judge-solution.md)** afterward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Judge Template\n",
    "\n",
    "Run the cell below to test your completed template. This loads your template from the activity file and evaluates the cache refactor scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your judge template with the cache refactor scenario\n",
    "from setup_utils import test_activity_3_4, get_refactor_judge_scenario\n",
    "\n",
    "print(\"üß™ Testing your evaluation template from activity-3.4-evaluation-templates.md...\")\n",
    "print(\"=\" * 70)\n",
    "judge_preview = test_activity_3_4(variables=get_refactor_judge_scenario())\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nüí° Review the verdict above. Does it match your expectations?\")\n",
    "print(\"   - If TODOs remain, complete your template in the activity file\")\n",
    "print(\"   - If scores seem off, adjust your criteria and re-run this cell\")\n",
    "print(\"   - To see the reference solution, check solutions/activity-3.4-judge-solution.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Test with custom scenario\n",
    "# \n",
    "# If you want to test your judge with different code, modify the variables below\n",
    "# and run this cell. Otherwise, the cell above tests with the standard scenario.\n",
    "\n",
    "from setup_utils import test_activity_3_4\n",
    "\n",
    "custom_variables = {\n",
    "    'service_name': 'TODO - Your Service Name',\n",
    "    'refactor_brief': 'TODO - What was refactored?',\n",
    "    'code_before': \"\"\"\n",
    "# TODO: Paste original code here\n",
    "\"\"\",\n",
    "    'code_after': \"\"\"\n",
    "# TODO: Paste refactored code here\n",
    "\"\"\",\n",
    "    'refactor_goal': 'TODO - What was the goal?',\n",
    "    'test_summary': 'TODO - Test results',\n",
    "    'analysis_findings': 'TODO - Linter/static analysis output',\n",
    "    'critical_regression': 'TODO - Any known regression?',\n",
    "    'security_findings': 'TODO - Security scan results',\n",
    "    'escalation_channel': '#your-channel',\n",
    "    'ai_refactor_output': \"\"\"\n",
    "# TODO: Paste the AI's explanation of the refactor\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(\"üß™ Testing with custom scenario...\")\n",
    "print(\"‚ö†Ô∏è Make sure to replace all TODO values above before running!\")\n",
    "print(\"=\" * 70)\n",
    "judge_result = test_activity_3_4(variables=custom_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluate Your Judge Template\n",
    "\n",
    "<div style=\"background:#f0f9ff; border-left:4px solid #0ea5e9; padding:16px; border-radius:6px; margin:20px 0; color:#000000;\">\n",
    "<strong style=\"color:#0c4a6e;\">üí° Want feedback on your judge template?</strong><br><br>\n",
    "\n",
    "Use <code style=\"color:#dc2626; background-color:#f1f1f1; padding:2px; font-family:Consolas,'courier new';\">evaluate_prompt()</code> to get comprehensive automated feedback (same evaluation system from Section 3.2):\n",
    "\n",
    "- **Traditional Metrics (40%)**: Pattern detection\n",
    "- **AI Evaluation (40%)**: Quality scores with confidence levels\n",
    "- **Semantic Similarity (20%)**: Comparison with reference solution\n",
    "\n",
    "<strong>üìö For details on evaluation and Confidence Scores, see Section 3.2.</strong>\n",
    "</div>\n",
    "\n",
    "**Run the cell below to evaluate your judge template!** ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Evaluate your Activity 3.4 judge template\n",
    "from setup_utils import evaluate_prompt, extract_template_from_activity, get_refactor_judge_scenario\n",
    "\n",
    "template, error = extract_template_from_activity('activities/activity-3.4-evaluation-templates.md')\n",
    "\n",
    "if error:\n",
    "    print(error)\n",
    "else:\n",
    "    # Define the same variables used in cell 12 for substitution\n",
    "    # These match what test_activity_3_4() uses to fill the template placeholders\n",
    "    variables = get_refactor_judge_scenario()\n",
    "    \n",
    "    # Substitute variables in template (same logic as test_activity uses internally)\n",
    "    print(\"üîÑ Substituting template variables...\")\n",
    "    substituted_template = template\n",
    "    for key, value in variables.items():\n",
    "        placeholder = \"{{\" + key + \"}}\"\n",
    "        substituted_template = substituted_template.replace(placeholder, str(value))\n",
    "    \n",
    "    print(\"üìñ Evaluating your Activity 3.4 judge template...\")\n",
    "    print(\"‚è≥ This will take ~30 seconds\\n\")\n",
    "    \n",
    "    evaluate_prompt(\n",
    "        messages=substituted_template,  # ‚úÖ Now fully substituted with actual content\n",
    "        activity_name=\"Activity 3.4: Evaluation Template\",\n",
    "        expected_tactics=[\n",
    "            \"Role Prompting\",\n",
    "            \"Structured Inputs\",\n",
    "            \"Output Format Specification\",\n",
    "            \"Chain-of-Thought\",\n",
    "            \"Evaluation Rubric\",\n",
    "            \"Weighted Criteria\"\n",
    "        ],\n",
    "        activity_file='activities/activity-3.4-evaluation-templates.md',\n",
    "        compare_with_reference=True,\n",
    "        track_progress=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Evaluation complete!\")\n",
    "    print(\"Next: Run view_progress() below to see your improvement!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Track Your Progress\n",
    "\n",
    "After completing the evaluation above, run the cell below to see your learning journey:\n",
    "\n",
    "- üìä All your evaluation attempts for this section  \n",
    "- üìà Your improvement over time\n",
    "- üèÜ Achievement status (scores ‚â• 80 earn **SKILLS ACQUIRED** badge!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä VIEW YOUR PROGRESS\n",
    "# Run this cell anytime to see your evaluation history and improvement\n",
    "\n",
    "from setup_utils import view_progress\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä YOUR SECTION 3.4 PROGRESS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "view_progress(\"Activity 3.4: Evaluation Template\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"üí° TIP: Scored ‚â• 80? You've mastered evaluation templates!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### What's Next: From Manual Judging to Systematic Evals\n",
    "\n",
    "**You just tested your judge on one scenario.** To use this in production, you need **systematic evaluations** that track judge performance over time.\n",
    "\n",
    "#### Why Evals Matter\n",
    "\n",
    "Manual testing validates one case. **Evals** validate your judge across hundreds of cases and track metrics:\n",
    "- **Accuracy**: Does your judge correctly identify good vs bad refactors?\n",
    "- **False positives**: How often does it block acceptable changes?\n",
    "- **Consistency**: Does rubric v2 improve on v1?\n",
    "\n",
    "**Learn why evals are critical:** [Why LLM Evals Matter](https://www.youtube.com/watch?v=vygFgCNR7WA&list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S)\n",
    "\n",
    "#### Production Eval Platforms\n",
    "\n",
    "Scale your judge with evaluation platforms:\n",
    "\n",
    "- **[OpenAI Platform Evals](https://platform.openai.com/docs/guides/evals)**: Dashboard-based systematic evaluation with datasets and metrics\n",
    "- **[Anthropic Evaluation Tool](https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool)**: Console-based prompt testing with side-by-side comparison\n",
    "\n",
    "#### Quick Start\n",
    "\n",
    "1. **Build an eval dataset**: Collect 10-20 refactors with known verdicts\n",
    "2. **Run systematic evals**: Test your judge template against the dataset\n",
    "3. **Track metrics**: Measure accuracy, iterate on rubric weights\n",
    "4. **Compare models**: Test if GPT-4o vs Claude performs better as judge\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"padding:16px; background:linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius:10px; color:#fff; text-align:center; box-shadow:0 4px 15px rgba(102,126,234,0.3);\">\n",
    "  <strong style=\"font-size:1.05em;\">üéâ Excellent work! You've completed the advanced evaluation section.</strong><br>\n",
    "  <span style=\"font-size:0.92em; opacity:0.95; margin-top:4px; display:block;\">Take a moment to reflect on what you've learned before moving forward.</span>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"padding:20px; background:linear-gradient(135deg, #a8edea 0%, #fed6e3 100%); border-radius:10px; text-align:center; box-shadow:0 4px 15px rgba(168,237,234,0.3); margin-top:24px;\">\n",
    "  <div style=\"font-size:2em; margin-bottom:10px;\">üéä</div>\n",
    "  <div style=\"font-size:1.3em; font-weight:700; color:#2d3748; margin-bottom:8px;\">Congratulations on Completing Module 3!</div>\n",
    "  <div style=\"font-size:0.95em; color:#2d3748; opacity:0.9;\">You're now equipped to build, evaluate, and deploy production-ready AI automation workflows.</div>\n",
    "</div>\n",
    "\n",
    "\n",
    "### What You Built\n",
    "You've mastered all core sections of Module 3, learning to build production-ready prompt templates and quality gates for AI-powered development workflows.\n",
    "\n",
    "**Section 3.2:** Code Review Automation ‚Äî Comprehensive review templates with severity classification  \n",
    "**Section 3.3:** Test Generation Automation ‚Äî Requirements-to-tests with ambiguity detection  \n",
    "**Section 3.4:** Evaluation Templates ‚Äî Weighted rubrics for automated quality gates\n",
    "\n",
    "### Key Skills Acquired\n",
    "\n",
    "<div style=\"background:#fff; border:2px solid #e5e7eb; padding:16px; border-radius:8px; margin:16px 0; color:#000000;\">\n",
    "\n",
    "**üéØ Template Design**\n",
    "- ‚úÖ Multi-tactic stacking (role + structure + reasoning + output)\n",
    "- ‚úÖ Variable substitution for reusable templates\n",
    "- ‚úÖ Command-style organization for automation\n",
    "\n",
    "**‚öñÔ∏è Evaluation Systems**\n",
    "- ‚úÖ Weighted rubric design (accuracy, completeness, actionability)\n",
    "- ‚úÖ Decision thresholds (Accept/Revise/Reject)\n",
    "- ‚úÖ Evidence-based reasoning with confidence scores\n",
    "\n",
    "**üîÑ Production Workflows**\n",
    "- ‚úÖ Multi-dimensional code review (security, performance, quality)\n",
    "- ‚úÖ Systematic test specification (ambiguities ‚Üí coverage ‚Üí specs)\n",
    "- ‚úÖ Automated quality gates with LLM judges\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"padding:12px; background:#dbeafe; border-radius:6px; border-left:4px solid #3b82f6; color:#1e40af; margin-top:16px; margin-bottom:16px;\">\n",
    "<strong>üìù Skills Demonstrated</strong><br><br>\n",
    "If you scored ‚â• 80 on the activities, you've demonstrated the ability to:\n",
    "<ul style=\"margin:8px 0 0 0;\">\n",
    "<li>Design production-ready prompt templates from scratch</li>\n",
    "<li>Combine multiple tactics into reliable automation workflows</li>\n",
    "<li>Build evaluation rubrics that scale to hundreds of outputs</li>\n",
    "<li>Implement quality gates with clear decision thresholds</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "<div style=\"background:#fff; border:2px solid #e5e7eb; padding:16px; border-radius:8px; margin:16px 0; color:#000000;\">\n",
    "\n",
    "**Evaluation Platforms:**\n",
    "- **[OpenAI Platform Evals](https://platform.openai.com/docs/guides/evals)** ‚Äî Dashboard-based systematic evaluation with datasets\n",
    "- **[Anthropic Evaluation Tool](https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool)** ‚Äî Console-based prompt testing\n",
    "\n",
    "**Production Patterns:**\n",
    "- **[AWS Anthropic Patterns](https://github.com/aws-samples/anthropic-on-aws/tree/main/advanced-claude-code-patterns)** ‚Äî Production command patterns\n",
    "- **[OpenAI GPT-5 Guide](https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide)** ‚Äî Latest prompting techniques\n",
    "\n",
    "**Learning Resources:**\n",
    "- **[Why LLM Evals Matter](https://www.youtube.com/watch?v=vygFgCNR7WA)** ‚Äî Video series on evaluation strategies\n",
    "- **[Evaluation Challenges](https://youtu.be/vBJF2sy1Pyw)** ‚Äî Understanding evaluation pitfalls\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"padding:24px 28px; background:linear-gradient(135deg, #10b981 0%, #059669 100%); border-radius:12px; box-shadow:0 4px 20px rgba(16,185,129,0.4); margin-top:24px; color:#fff;\">\n",
    "  <div style=\"text-align:center; margin-bottom:20px;\">\n",
    "    <div style=\"font-size:3em; margin-bottom:8px;\">üéì</div>\n",
    "    <div style=\"font-size:1.4em; font-weight:700; margin-bottom:8px;\">Course Complete!</div>\n",
    "    <div style=\"font-size:1.05em; opacity:0.95; line-height:1.5;\">You've mastered the Advanced Prompt Engineering for Developers course</div>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"background:rgba(255,255,255,0.15); border-radius:8px; padding:20px; margin:20px 0; backdrop-filter:blur(10px);\">\n",
    "    <div style=\"font-size:0.95em; font-weight:600; margin-bottom:12px; text-transform:uppercase; letter-spacing:1px;\">üèÜ What You've Accomplished</div>\n",
    "    <div style=\"font-size:0.92em; line-height:1.7; opacity:0.95;\">\n",
    "      <strong>Module 1:</strong> Foundations & prompt anatomy<br>\n",
    "      <strong>Module 2:</strong> Core tactics (roles, structure, reasoning, patterns)<br>\n",
    "      <strong>Module 3:</strong> Production workflows (code review, test generation, evaluation templates)<br>\n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"background:rgba(255,255,255,0.15); border-radius:8px; padding:20px; margin:20px 0; backdrop-filter:blur(10px);\">\n",
    "    <div style=\"font-size:0.95em; font-weight:600; margin-bottom:12px; text-transform:uppercase; letter-spacing:1px;\">üöÄ You're Now Ready To</div>\n",
    "    <div style=\"font-size:0.92em; line-height:1.8; opacity:0.95;\">\n",
    "      ‚úì Design production-ready prompts that scale across your team<br>\n",
    "      ‚úì Build automated workflows with multi-tactic prompt templates<br>\n",
    "      ‚úì Implement quality gates using evaluation rubrics<br>\n",
    "      ‚úì Integrate prompt patterns into your development environment<br>\n",
    "      ‚úì Lead prompt engineering initiatives at your organization\n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"background:rgba(255,255,255,0.15); border-radius:8px; padding:20px; margin:20px 0; backdrop-filter:blur(10px);\">\n",
    "    <div style=\"font-size:0.95em; font-weight:600; margin-bottom:12px; text-transform:uppercase; letter-spacing:1px;\">üí° Continue Your Journey</div>\n",
    "    <div style=\"font-size:0.92em; line-height:1.8; opacity:0.95;\">\n",
    "      ‚Ä¢ Apply these patterns to your real projects<br>\n",
    "      ‚Ä¢ Share templates with your team<br>\n",
    "      ‚Ä¢ Iterate and refine based on production feedback<br>\n",
    "      ‚Ä¢ Build systematic evaluation datasets<br>\n",
    "      ‚Ä¢ Contribute to the prompt engineering community\n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"text-align:center; margin-top:24px; padding-top:20px; border-top:2px solid rgba(255,255,255,0.2);\">\n",
    "    <div style=\"font-size:1.1em; font-weight:600; margin-bottom:8px;\">Thank you for completing this course!</div>\n",
    "    <div style=\"font-size:0.9em; opacity:0.9;\">Keep building, experimenting, and pushing the boundaries of what's possible with AI.</div>\n",
    "  </div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
